---
layout: default
title: 09. S3
parent: aws
---
# 09. S3
---
## 소개

무한하게 확장할 수 있는 스토리지
백업과 스토리지로 사용한다

재해 복구 용도로도 사용한다. 예를들어 리전이 다운되는 경우 데이터를 다른 리전으로 이동시켜야 한다. 이런 경우 데이터는 어딘가에 백업되어야하는데 이때 사용한다?

아카이브용으로도 사용한다. S3에 파일을 아카이브해 두면 추후 매우 손쉽게 검색할 수 있다.

애플리케이션을 호스팅하고 동영상 파일이나 이미지 등 미디어를 호스팅할 수 있다.

데이터 레이크를 보유하여 다량의 데이터를 저장하고 빅 데이터 분석을 수행하기 위해서도 사용한다.

>데이터 레이크(Data Lake)
>정형, 반정형, 비정형 데이터를 저장하는 중앙 집중식 저장소입니다. 데이터 레이크는 데이터를 기본 형식으로 저장할 수 있으며, 크기 제한을 무시하고 다양한 데이터를 처리할 수 있습니다.

정적 웹사이트를 호스팅하기 위해서도 사용할 수 있다.
이게 cloudFront 인건가?

S3는 파일을 버킷에 저장하는데, 버킷은 상위 레벨 디렉토리로 표시된다. ????

S3 버킷의 파일은 객체라고한다.

버킷의 이름은 계정에 있는 모든 리전과 AWS에 존재하는 모든 계정에서 고유해야한다.
하지만 버킷은 리전 수준에서 정의된다.

S3 버킷에는 명명 규칙이있는데 기억할 필요는 없지만 알아두면 좋다.

S3 그 자체로는 디렉토리의 개념이 없다.
핵심은 키

S3 최대 객체 크기는 5TB이다

파일 업로드시 5GB보다 크다면 나눠서 업로드해야한다.

5TB의 경우는 1000개로 나눠서 업로드 필요.

객체에는 메타데이터도 있어서 필요에 따라 사용할 수 있다.

---

## 보안 (S3 버킷 정책)


User-Based
- 사용자 기반 (IAM 사용)
 Resource-Based
- 리소스 기반
- S3 버킷 정책 사용하여 전체 버킷 규칙을 세울 수 있다.
- 특정 사용자가 들어올 수 있게 하거나 다른 계정의 사용자를 허용할수 있게 설정하는것을 교차 계정이라고한다
- 이는 S3버킷을 공개로 만드는 방법이기도 하다
- ACL
	- object Access Control List
		- 객체 액세스 제어 목록
		- 세밀한 보안이며 비활성화할 수 있다
	- Bucket Access Control List 
		- 훨씬 덜 일반적이다.
		- 비활성화할 수 있다.

S3 버킷의 보안관리는 일반적으로 버킷 정책을 사용하는것이다.

보안을 관리할 수 있는 방법은 암호화 키를 사용하여 객체를 암호화 하는것이다.

JSON기반 보안 정책 작성

Principal이 `*` 인 경우 모든 사용자를 허용한다

S3 버킷 정책을 설정하여 공개로 만들더라도 보안키 설정이 되어있다면 버킷은 결코 공개되지않는다.

---

## S3 정적 웹사이트

버킷에서 공개 읽기가 활성화 되지 않은경우 이것은 동작하지않는다. (403 FOrbidden 오류가 나는경우 해당 케이스)

웹사이트 URL은 버킷이름과 AWS리전에 따라 달라진다

---

## S3 버전관리

S3는 파일을 버전 관리할 수 있다
버킷 수전에서 활성화 해야한다
사용자가 파일을 업로드할 때마다 선택키에서 해당 파일의 버전이 생성된다
동일한 키를 업로드하고 해당 파일을 덮어쓰는 경우 버전 2 버전 3 등을 생성하게 된다

버전 관리를 활성화하기 전에 버전 관리가 적용되지 않은 모든 파일은 null버전을 갖게된다
또 버전 관리르 중단해도 이전 버전을 삭제하지는 않는다

---

## S3 복제

- 교차 리전 복제 CRR
- 같은 리전으로 복제 SRR

소스 버킷과 복제 대상 버킷 둘 모두 버전 관리 기능이 활성화되어야한다
CRR은 이름 그대로 두 리전이 달라야 하고, SRR은 같은 리전이여야한다
복제는 비동기식으로 이루어진다
복제 과정은 백그라운드에서 이루어진다
복제 기능이 정상적으로 실행되려면 , S3에 올바른 IAM권한 (읽기 쓰기)를 부여해야한다.

CRR은 컴플라이언스(법규나 내부 체제 관리), 다른 리전에 있어 발생할 수 있는 지연시간을 줄이는 경우 에 사용한다

SRR은 버킷간의 로그를 통합할때나 개발 환경이 별도로 있어 운영 환경과 개발 환경간의 실시간 복제를 필요로 할 떄 사용한다

복제를 활성화한 후에는 새로운 객체만 복제 대상이 된다.
기존의 객체를 복제하려면 S3 배치 복제 기능을 사용해야한다.
기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능이다

작업을 삭제하려면?? 이게뭔소리임
소스 버킷에서 대상 버킷으로 삭제 마커를 복제하면 된다
버전 ID로 삭제하는 경우 버전 ID는 복제되지 않는다

체이닝 복제는 불가능하다 
1번 버킷이 2번에 복제되어있고, 2번 버킷이 3번에 복제돼있는 경우 1번 버킷의 객체가 3번 버킷으로 복제되지 않는다.
?????

---

### S3 Storage Classes

1. Amazon S3 Standard
	- 지연시간이 짧고 처리량이 높음
	- AWS에서 두개의 기능장애를 동시에 버틸 수 있다
	- 빅데이터 분석, 모바일, 게임 애플리케이션, 콘텐츠 배포에 사용
	- 가용성은 99.99%
2. Amazon S3 Infrequent Access
	- 자주 액세스하지는 않지만 필요한 경우 빠르게 액세스해야 하는 데이터
	- standard보다 비용이 적게 들지만 검색 비용이 발생
	- 가용성은 99.9%
	- 사용 사례는 재해복구와 백업
3. Amazon S3 One Zone - Infrequent Access
	- 단인 AZ 내에서는 높은 내구성을 갖지만 AZ가 파괴된 경우 데이터를 잃게 된다.
	- 가용성은 99.5%
	- 사용 사례는 온프레미스 데이터를 2차 백업하거나 재생성 가능한 데이터를 저장하는데 쓴다.
4. Amazon S3 Intelligent Tiering
	- 사용자 패턴에 따라 액세스된 티어 간에 객체를 이동할 수 있게 해준다
	- 소액의 월별 모니터링 비용과 티어링 비용이 발생한다
	- 검색 비용이 없다
	- FrequentAccess 티어
		- 자동?, 기본 티어
	- InfrequestAccess 티어
		- 30일 동안 액세스 하지 않는 객체 전용 티어
	- Archive Instant Access 티어
		- 자동?,
		- 90일 동안 액세스 하지 않는 티어
	- Archive Access 티어
		- 선택 사항
		- 90일에서 700일 이상까지 구성할 수 있다
	- Deep Archive Access 티어
		- 180일에서 700일 이상 액세스 하지 않는 객체 티어
	- 알아서 객체를 이동시켜주기 떄문에 편하게 스토리지를 관리할 수 있다. (위의 자동이란건 자동으로 파일을 옮겨준다는 뜻이구나..!)
	- 

Glacier 종류
- Glacier 는 콜드 스토리지이다
- 아카이빙과 백업을 위한 저비용 객체 스토리지
1. Glacier Instant Reterieval
	- 밀리초 단위로 검색 가능
	- 분기에 한번 액세스하는 데이터에 아주 적합
	- 최소 보관 기간이 90일 이내
	- 백업이지만 밀리초 이내에 액세스 하는경우에 적합
2. Glacier Flexible Retrieval
	- 3가지 옵션
	- Expedited
		- 데이터를 1~5분 이내에 받을 수 있다
	- Standard
		- 데이터를 돌려받는데 3~5시간 소요
	- Bulk
		- 무료이지만 데이터를 돌려받는데 5~12시간 소요
	- 최소 보관 기간 90일
3. Glacier Deep Archive
	- 장기보관을 위해 사용
	- 데이터 검색 시간
		- standard - 12시간
		- bulk - 48시간
	- 가장 저렴한 비용
	- 최소 보관 기간 180일



스토리지 클래스 간에 객체를 자동으로 이동할 수도 있다

내구성
- S3로 인해 객체가 손실되는 획수
- 99.99999999% 보장
가용성
- 서비스가 얼마나 용이하게 제공되는지를 나타냄
- 스토리지 클래스마다 다름
- standard는 1년에 53분정도만 서비스를 사용할 수 없다


---

다른 스토리지 클래스 간에 객체를 옮기는 방법

Amazon S3 Standard
Amazon S3 Infrequent Access
Amazon S3 Intelligent Tiering
Amazon S3 One Zone - Infrequent Access
Glacier Instant Reterieval
Glacier Flexible Retrieval
Glacier Deep Archive

위에서 아래 로 클래스 간 객체를 옮길 수 있다.

수작업으로 옮길 수도 있지만, 라이프사이클 규칙을 이용해서 자동으로 옮길 수도있다


### 라이프사이클 룰

Transition Action
	다른 스토리지 클래스로 이전하기 위해 객체를 설정
	예를들어 생성된지 60일 후에 Standard 클래스로 이전
	또는 6개월 후에 Glacier로 이전하여 아카이브화
Expiration actions
	일정 시간뒤에 만료되어서 객체를 삭제할 수 있다
	
특정 접두어에 대해 지정도 가능하다
태그 또한 가능

---

## 요청자 지불

스토리지 비용은 버킷 소유자가 지불하고
다운로드시 발생하는 네트워킹 비용은 요청자에게 청구할 수 있다
이를위해서는 요청자는 익명이여서는 안된다
요청자가 AWS에서 인증을 받아야한다

---

## 이벤트 알림

이벤트는 예를 들어 객체가 생성되었거나 객체가 삭제되었거나 객체가 복구되었거나 복제되는 것 등을 말한다

이벤트들은 필터링 될 수 있다

이벤트를 만들고 그걸 몇몇 대상에 전송할 수 있다

대상은 SNS토픽이나 SQS Queue, 람다 함수 등이 될 수 있다

이벤트 알림이 작동하려면 IAM권한을 갖고 있어야 한다
(SNS토픽에 전송하려고 한다면 SNS 리소스 정책이라는것을 첨부해야함, 람다를 호출하려면 람다 호출 권한을 가져야함)

이벤트는 S3 버킷으로 갈것이고, 모든 이벤트는 결국 Amazon EventBridge로 가게된다

이벤트 브릿지에서는 규칙을 설정할 수 있다
이 규칙들로 18가지 AWS 서비스에 전송할 수 있다

---

## 성능

S3는 요청이 아주 많을 때 자동으로 확장된다

S3로부터 첫 번째 바이트를 수신하는데 지연시간도 100~200밀리초 사이로 아주 짧다

초당 3,500개의 PUT/COPY/POST/DELETE
초당 5,500개의 GET/HEAD 요청을 지원한다

>접두사당 초당 의미
>첫 번째 객체의 위치는 bucket/folder1/sub1/file입니다
>bucket과 file 사이에 있는 것이 접두사가 됩니다
>/folder1/sub1이 되겠죠


### 성능 최적화

100MB가 넘는 파일은 멀티파트 업로드를 사용하는것이 좋다
5GB가 넘는 파일은 반드시 사용해야한다

멀티파트 업로드는 업로드를 병렬화 하여 전송속도를 높여 대역폭을 최대화할 수 있다.


### 전송 가속화

파일을 AWS 엣지 로케이션으로 전송해서 전송 속도를 높이고 데이터를 대상 리전에 있는 S3 버킷으로 전달한다

엣지 로케이션은 리전수보다 많다


파일을 수신하고 파일을 읽는 가장 효율적인 방법은?
`S3 바이트 범위 가져오기`

파일에서 특정 바이트 범위를 가져와서 GET요청을 병렬화 하는 방법

특정 바이트 범위에서 가져오는데 실패하는 경우 더 작은 바이트 범위에서 재시도하여 실패의 경우 복원력이 높다

---

## Select & Glacier Select

S3에서 파일을 검색할 때 검색한 다음에 필터링하면 너무 많은 데이터를 검색하게된다

대신 서버 측 필터링을 수행하면 비용을 줄일 수 있다
SQL문에서 간단히 행과 열을 사용해 필터링할 수 있다
네트워크 전송이 줄어들기 때문에 데이터 검색과 필터링에 드는 클라이언트 측의 CPU 비용도 줄어든다

select로 CSV파일을 가져오는 방법도 있다
데이터 크기가 훨씬 작아 저렴하다

---

## Batch Operations

단일 요청으로 S#객체에서 대량 작업을 수행하는 서비스

한번에 많은 S3 객체의 메타 데이터와 프로퍼티를 수정할 수 있다

S3버킷 간에 객체를 복사할 수 있다

S3 버킷 내 암호화되지 않은 모든 객체를 암호화할 수 있다

ACL이나 태그를 수정할 수 있다

S3 Clacier에서 한번에 많은 객체를 복원할 수 있다

Lambda 함수를 호출해서 S3 Batch Operations의 모든 객체에서 사용자 지정 작업을 수행할 수 있다

S3 Batch Operations를 사용하면 재시도를 관리할 수 있다
진행 상황을 추적하고 작업 완료 알림을 보내고 보고서 생성 등을 할 수 있다

배치에 전달할 객체 목록은 S3 Inventory라는 기능을 사용하여 객체 목록을 가져오고 S3 Select를 사용하여 객체를 필터링한다
객체 목록을 얻은 다음 
S3 Batch Operations에 수행할 작업, 매개 변수와 함께 객체 목록을 전달한다


---

## 보안

4가지 방법

SSE (Server Side Encryption)
- SSE-S3 : S3에서 관리하는 키를 이용한 서버측 암호회
	- AWS가 처리하고 관리하는 소유키를 사용해서 암호화
	- 키에 절대 접근 불가능하다
	- AWS에 의해 서버측에서 암호화 적용
	- 보안유형 AES-256
	- SSE-S3 메커니즘을 이용하기 위해 객체를 암호화 하도록 헤더를 "x-amz-server-side-encrytion":"AES256"이라고 설정해야한다
- SSE-KMS : KMS키를 이용해서 암호화 키를 관리
	- 키 관리 서비스를 이용해 직접 키를 관리한다
	- KMS의 장점은 사용자가 키를 통제할 수 있다는것
	- CloudTrail을 이용해서 키 사용을 검사할 수 있다
	- CloudTrail AWS안에서 일어나는 모든걸 로깅하는 서비스
	- 헤더를 "x-amz-server-side-encrytion":"aws:kms"이라고 설정해야한다
	- KMS 키에도 접근 권한이 있어야 암호를 해제할 수 있다
	- 제약사항
		- 복호화를 위해 KMS api를 호출해야한다.
		- 처리량이 아주 많고 모든게 KMS키로암호회 되어있다면 스로틀링이 걸릴 수 있다
- SSE-C : 고객이 제공한 키를 사용
	- 파일과 키를 함께 업로드한다
CSE (Client Side Encryption)
- 암호화 라이브러리를 사용하면 쉽게 구현
- 클라이언트가 직접 암호화 한다음 S3에 업로드한다

기본값 암호화는 SSE-S3로 제공하지만 버킷 정책으로 암호화를 강제할 수 있다.
### 전송중 암호화

S3 버킷은 기본적으로 2개의 엔드포인트가 존재한다
암호화가 되지 않는 HTTP
암호화가 제공되는 HTTPS
S3는 HTTPS를 권고한다

전송 중 암호화를 강제하는방법
- 버킷 정책 사용
- S3 버킷에 버킷 정책 첨부하고
- aws:SecureTransport가 false라면 GetObject 작업을 거부한다
- 해당 옵션으로 암호화 연결을 사용하지 않는다면 차단된다

---

## CORS

오리진은 체계(프로토콜)와 호스트(도메인)와 포트로 구성된다

https://www.example.com:443
프로토콜 도메인 포트의 예

CORS는 웹 브라우저 기반 보안 메커니즘이다
메인 오리진을 방문하는 동안 다른 오리진에 대한 요청을 허용하거나 거부한다
오리진이 같다는건 프로토콜 도메인 포트가 같다라는 뜻


#### S3에 적용

클라이언트가 S3버킷에서 교차 오리진 요청을 하면 정확한 CORS 헤더를 활성화 해야한다

시험에 나옴
이 작업을 빠르게 수행하려면 특정 오리진을 허용하거나 `*`를 붙여 모든 오리진을 허용한다

---
### MFA

멀티팩터 인증

사용자가 장치에서 코드를 생성하도록 강제한다

MFA는 객체 버전을 영구적으로 삭제할 때 필요하다
영구 삭제에 대한 보호 설정

버킷에서 버저닝을 중단할 때도 필요하다

루트 계정만이 MFA delete를 활성화하거나 비활성화할수있다

---

### S3 Access Logs

어떤 계정이든 S3로 보낸 모든 요청은 승인 또는 거부 여부와 관계없이 기록된다

해당 데이터는 Amazon Athena 같은 데이터 분석 도구로 분석할 수 있다

로깅 버킷은 같은 AWS리전에 있어야한다

액세서 로그를 활성화 하면 모든 요청이 로깅 버킷에 기록된다

특정 로그 형식이 있다

주의사항
- 절대 로깅 버킷을 모니터링하는 버킷과 동일하게 설정하면 안된다.
- 동일하게 설정하면 로깅 루프가 생성되고 무한이 반복되어 버킷의 크기가 기하급수적으로 증가한다

---

## Pre-Signed URLs

URL에는 만료 기한이 있다
S3 콘솔을 사용하면 최대 12시간
CLI를 하면 168시간까지 사용할 수 있다

서명된 URL을 생성할때 URL을 받는 사용자는 URL을 생성한 사용자의 GET 또는 PUT에 대한 권한을 상속한다

---

### S3 Glacier Vault Lock
볼트 잠금

WORM모델을 채용하기 위해 Clacier 볼트를 잠근다

WORM은 한번 쓰고 여러번 읽는다는 뜻이다

객체를 가져와서 S3 Clacier에 넣은다음 수정하거나 삭제할 수 없도록 잠그는것

잠금 정책을 설정하고 잠근 후에는 누구도 변경하거나 삭제할 수 없다



유사한 옵션으로
### S3 객체 잠금
이 있다

조금더 복잡하다

S3 객체 잠금을 활성화 하려면 먼저 버저닝을 활성화 해야한다

잠금은 전제가 아닌 버킷 내의 모든 객체에 각각 적용할 수 있다

특정 객체 버전이 특정 시간동안 삭제되는걸 차단할 수 있다

객체 잠금에는 보존 모드가 있다
 
#### 보존모드 

규정 준수 모드
- S3 Glacier 볼트 잠금과 매우 유사
- 사용자를 포함한 누구도 객체 버전을 덮어쓰거나 삭제할 수 없다
- 규정 준수 모드에서는 누구도 객체를 변경할 수 없다
- 보존 모드 자체도 변경할 수 없다
- 보돈 기간도 단축할 수 없다

거버넌스 보존 모드
- 좀 더 유연하다
- 객체 버전을 덮어쓰거나 삭제하거나 로그 설정을 변경할 수 없다
- 하지만 관리자 같은 일부 사용자는 IAM을 통해 부여받은 특별한 권한으로 보존기간을 변경하거나 객체를 바로 삭제할 수 있다

객체에 법적 보존 상태를 설정할 수 있다
- S3 버킷 내 모든 객체를 무기한으로 보존한다
- s3:putObjectLegalHold IAM권한을 가진 사용자는 어떤 객체든 법전 보존을 설정하거나 제거할 수 있다

---


### 엑세스 포인트

사용자의 데이터가 많아질수록 관리하기 어렵다

해소방법은?
엑세스 포인트

예를들어 재무 관련된 파일들을 /finance 라는 접두어에 읽기와 쓰기 액세스 권한을 부여한다

엑세스 포인트=에는 각자의 보안을 갖고있다

특정 사람들만 재무 부문에 접속할 수 있도록 하게된다

이렇게 해서 실제로 S3 버킷에 대한 엑세스를 스케일링 할 수 있따

엔드포인트는 각자의 DNS 이름을 갖게 된다

---

## S3 Object Lambda
액세스 포인트에는 또 다른 활용 사례가 있다. S3 객체 람다

S3버킷이 있는데 호출자 애플리케이션이 객체를 받기 직전에 객체를 수정하려는 경우

버킷을 복제해서 버킷에 각 객체의 다른 버전을 갖는 대신에 S3 객체 람다를 사용할 수 있다

람다를 거치는 과정에서 요청자에게 데이터를 전달하고 객체를 삭제하거나 수정한다

이를 위해 S3 액세스 포인트가 필요하다

